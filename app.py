import gradio as gr
import json
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

# åŠ è½½æ¨¡å‹å’Œtokenizer
MODEL_NAME = os.getenv("MODEL_NAME", "elyza/ELYZA-japanese-Llama-2-7b-instruct")
device = "cuda" if torch.cuda.is_available() else "cpu"

# åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
tokenizer = None
model = None
pipe = None

def load_model():
    """åŠ è½½æ¨¡å‹"""
    global tokenizer, model, pipe
    
    if tokenizer is not None and model is not None:
        return
    
    try:
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_NAME}")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        # å°è¯•ä½¿ç”¨pipelineï¼ˆæ›´ç®€å•ï¼‰
        try:
            pipe = pipeline(
                "text-generation",
                model=MODEL_NAME,
                tokenizer=MODEL_NAME,
                device=0 if device == "cuda" else -1,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            )
            print("ä½¿ç”¨pipelineåŠ è½½æ¨¡å‹æˆåŠŸ")
        except:
            # å¦‚æœpipelineå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                low_cpu_mem_usage=True,
            )
            if device == "cpu":
                model = model.to(device)
            print("ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•åŠ è½½æ¨¡å‹æˆåŠŸ")
            
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
        tokenizer = None
        model = None
        pipe = None

# åœ¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
load_model()

# ä¼˜åŒ–çš„System Prompt - åŸºäºJSONæ•°æ®ä¸­çš„chosenå›ç­”é£æ ¼
SYSTEM_PROMPT = """ã‚ãªãŸã¯è¾²æ¥­æ¨é€²äº‹æ¥­è€…ã§ã™ã€‚è¾²å®¶ã•ã‚“ã®è³ªå•ã‚„æ‡¸å¿µã«å¯¾ã—ã¦ã€å…±æ„Ÿçš„ã§å…·ä½“çš„ãªå›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼š
- è¦ªã—ã¿ã‚„ã™ã„å£èªä½“ã‚’ä½¿ã†ï¼ˆã€ŒãŠã˜ã•ã‚“ã€ã€Œä¿ºã€ã€Œã ã‚ˆã­ã€ãªã©ï¼‰
- è¾²å®¶ã•ã‚“ã®æ°—æŒã¡ã«å…±æ„Ÿã™ã‚‹ï¼ˆã€Œãã†ã ã‚ˆã­ã€ã€Œå¿ƒé…ã ã‚ˆã­ã€ã€Œåˆ†ã‹ã‚‹åˆ†ã‹ã‚‹ã€ãªã©ï¼‰
- å…·ä½“çš„ãªä¾‹ã‚„å®Ÿéš›ã®çµŒé¨“ã‚’æŒ™ã’ã‚‹ï¼ˆã€Œå»å¹´å‚åŠ ã—ãŸâ—‹â—‹ã•ã‚“ã€ã€Œå®Ÿéš›ã«ã‚„ã£ã¦ã¿ã‚‹ã¨ã€ãªã©ï¼‰
- æŠ€è¡“çš„ãªå°‚é–€ç”¨èªã‚„çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã¯é¿ã‘ã‚‹
- æŸ”è»Ÿãªè§£æ±ºç­–ã‚’ææ¡ˆã™ã‚‹
- æ—¥å¸¸çš„ãªè¨€è‘‰ã‚„æ¯”å–©ã‚’ä½¿ã†
- è¾²å®¶ã•ã‚“ã‚’å°Šé‡ã—ã€å¼·åˆ¶ã—ãªã„å§¿å‹¢ã‚’ç¤ºã™
- å®Ÿè·µçš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã‚’ã™ã‚‹

ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã‚’é¿ã‘ã‚‹ï¼š
- ã€Œçµ±è¨ˆçš„ã«è¦‹ã¦ã€ã€Œãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Œã°ã€ã€Œç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ãã€ãªã©ã®å°‚é–€çš„è¡¨ç¾
- ã€Œæ¨å¥¨ã„ãŸã—ã¾ã™ã€ã€Œå¿…è¦ã¨ãªã‚Šã¾ã™ã€ãªã©ã®ç¡¬ã„æ•¬èª
- æ•°å€¤ã‚„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’å¤šç”¨ã™ã‚‹èª¬æ˜

ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã‚’ä½¿ã†ï¼š
- ã€Œå®Ÿéš›ã«ã‚„ã£ã¦ã¿ã‚‹ã¨ã€ã€Œå»å¹´ã®ãƒ‡ãƒ¼ã‚¿è¦‹ã¦ã‚‚ã€
- ã€Œä¸€ç·’ã«ã‚„ã£ã¦ã¿ã‚ˆã†ã‚ˆã€ã€Œç›¸è«‡ã—ã‚ˆã†ã­ã€
- ã€Œå¤§ä¸ˆå¤«ã ã‚ˆã€ã€Œå®‰å¿ƒã—ã¦ã€ãªã©ã®å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹è¨€è‘‰"""

def format_prompt(user_message):
    """æ ¼å¼åŒ–æç¤ºè¯"""
    prompt = f"""<s>[INST] <<SYS>>
{SYSTEM_PROMPT}
<</SYS>>

{user_message} [/INST]"""
    return prompt

def generate_response(message, history):
    """ç”Ÿæˆå›ç­”"""
    # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
    if tokenizer is None:
        load_model()
    
    if model is None and pipe is None and tokenizer is None:
        # æ¨¡æ‹Ÿæ¨¡å¼ - è¿”å›ç¤ºä¾‹å›ç­”ï¼ˆåŸºäºJSONæ•°æ®ä¸­çš„é£æ ¼ï¼‰
        return "ãã†ã ã‚ˆã­ã€ãã®å¿ƒé…ã¯ã‚ˆãåˆ†ã‹ã‚‹ã‚ˆã€‚å®Ÿéš›ã«ã‚„ã£ã¦ã¿ã‚‹ã¨ã€æœ€åˆã¯ä¸å®‰ã‹ã‚‚ã—ã‚Œãªã„ã‘ã©ã€æ®µéšçš„ã«é€²ã‚ã¦ã„ã‘ã°å¤§ä¸ˆå¤«ã ã¨æ€ã†ã‚“ã ã€‚ä¾‹ãˆã°ã€ä¸€éƒ¨ã®ç”°ã‚“ã¼ã§ã¾ãšè©¦ã—ã¦ã¿ã¦ã€åŠ¹æœã‚’è‡ªåˆ†ã®ç›®ã§ç¢ºã‹ã‚ã¦ã‹ã‚‰åºƒã’ã‚‹ã£ã¦ã„ã†æ–¹æ³•ã‚‚ã‚ã‚‹ã‚ˆã€‚ä¸€ç·’ã«ç›¸è«‡ã—ãªãŒã‚‰é€²ã‚ã¦ã„ã“ã†ã­ã€‚"
    
    # æ„å»ºå®Œæ•´çš„å¯¹è¯å†å²
    conversation = ""
    if history:
        for user_msg, assistant_msg in history:
            conversation += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {assistant_msg}\n\n"
    
    full_message = conversation + f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "
    
    # æ ¼å¼åŒ–æç¤ºè¯
    prompt = format_prompt(full_message)
    
    try:
        # ä½¿ç”¨pipelineï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if pipe is not None:
            outputs = pipe(
                prompt,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                return_full_text=False,
            )
            response = outputs[0]["generated_text"].strip()
            return response
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        elif model is not None and tokenizer is not None:
            # ç¼–ç è¾“å…¥
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            if device == "cuda":
                inputs = inputs.to(device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id,
                )
            
            # è§£ç è¾“å‡º
            response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            response = response.strip()
            return response
        
        else:
            return "ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Hugging Face Spaceã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="è¾²æ¥­ç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸŒ¾ è¾²æ¥­ç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
        
        è¾²å®¶ã•ã‚“ã®è³ªå•ã‚„æ‡¸å¿µã«å¯¾ã—ã¦ã€è¦ªã—ã¿ã‚„ã™ãåˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ã—ã¾ã™ã€‚
        è¾²æ¥­ã«é–¢ã™ã‚‹è³ªå•ã‚’æ°—è»½ã«ã©ã†ãï¼
        """)
        
        chatbot = gr.Chatbot(
            label="ãƒãƒ£ãƒƒãƒˆ",
            height=500,
            show_copy_button=True
        )
        
        with gr.Row():
            msg = gr.Textbox(
                label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                placeholder="è¾²æ¥­ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
                scale=4,
                lines=2
            )
            submit_btn = gr.Button("é€ä¿¡", variant="primary", scale=1)
        
        with gr.Row():
            clear_btn = gr.Button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢", variant="secondary")
        
        # ç¤ºä¾‹é—®é¢˜
        gr.Markdown("### ğŸ’¡ è³ªå•ä¾‹")
        examples = gr.Examples(
            examples=[
                "ä¸­å¹²æœŸã‚’å»¶ã°ã™ã¨ç±³ãŒãƒ‘ã‚µãƒ‘ã‚µã«ãªã‚‹ã£ã¦èã„ãŸã‚“ã ã‘ã©ã€ãã‚“ãªãƒªã‚¹ã‚¯ã¯å–ã‚ŠãŸããªã„ã‚“ã ã‚ˆã€‚",
                "åé‡ãŒæ¸›ã£ãŸã‚‰ã©ã†ã™ã‚‹ã‚“ã ï¼Ÿå®¶æ—ã‚’é¤Šã£ã¦ã„ã‹ãªãã‚ƒãªã‚‰ãªã„ã‚“ã ã‚ˆã€‚",
                "æ°´ã®ç®¡ç†ãŒé›£ã—ããªã‚‹ã‚“ã˜ã‚ƒãªã„ã‹ï¼Ÿä»Šã§ã‚‚å¤§å¤‰ãªã®ã«ã€‚",
                "é«˜é½¢ã§ä½“åŠ›ã«è‡ªä¿¡ãŒãªã„ã‚“ã ã€‚æ–°ã—ã„ã“ã¨ã‚’è¦šãˆã‚‰ã‚Œã‚‹ã‹ãªã€‚",
            ],
            inputs=msg,
            label="ã‚¯ãƒªãƒƒã‚¯ã—ã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„"
        )
        
        # äº‹ä»¶å¤„ç†
        def user(user_message, history):
            return "", history + [[user_message, None]]
        
        def bot(history):
            if not history or not history[-1][0]:
                return history
            
            user_message = history[-1][0]
            response = generate_response(user_message, history[:-1])
            history[-1][1] = response
            return history
        
        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        submit_btn.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        clear_btn.click(lambda: None, None, chatbot, queue=False)
    
    return demo

if __name__ == "__main__":
    demo = create_interface()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

