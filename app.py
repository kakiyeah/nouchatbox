"""
Hugging Face Inference APIã‚’ä½¿ç”¨ã™ã‚‹è»½é‡ç‰ˆ
ã“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã›ãšã€Hugging Faceã®Inference APIã‚’ä½¿ç”¨ã—ã¾ã™
"""
import gradio as gr
import os
import requests

# Hugging Face Inference APIã®è¨­å®š
# å¦‚æœé»˜è®¤æ¨¡å‹ä¸å¯ç”¨ï¼Œå¯ä»¥å°è¯•å…¶ä»–æ¨¡å‹ï¼š
# - elyza/ELYZA-japanese-Llama-2-7b-fast-instruct
# - cyberagent/calm2-7b-chat
# - meta-llama/Llama-2-7b-chat-hf (éœ€è¦è®¤è¯)
HF_API_URL = os.getenv("HF_API_URL", "https://api-inference.huggingface.co/models/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct")
HF_API_TOKEN = os.getenv("HF_API_TOKEN", "")

# ä¼˜åŒ–çš„System Prompt
SYSTEM_PROMPT = """ã‚ãªãŸã¯è¾²æ¥­æ¨é€²äº‹æ¥­è€…ã§ã™ã€‚è¾²å®¶ã•ã‚“ã®è³ªå•ã‚„æ‡¸å¿µã«å¯¾ã—ã¦ã€å…±æ„Ÿçš„ã§å…·ä½“çš„ãªå›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼š
- è¦ªã—ã¿ã‚„ã™ã„å£èªä½“ã‚’ä½¿ã†ï¼ˆã€ŒãŠã˜ã•ã‚“ã€ã€Œä¿ºã€ã€Œã ã‚ˆã­ã€ãªã©ï¼‰
- è¾²å®¶ã•ã‚“ã®æ°—æŒã¡ã«å…±æ„Ÿã™ã‚‹ï¼ˆã€Œãã†ã ã‚ˆã­ã€ã€Œå¿ƒé…ã ã‚ˆã­ã€ã€Œåˆ†ã‹ã‚‹åˆ†ã‹ã‚‹ã€ãªã©ï¼‰
- å…·ä½“çš„ãªä¾‹ã‚„å®Ÿéš›ã®çµŒé¨“ã‚’æŒ™ã’ã‚‹ï¼ˆã€Œå»å¹´å‚åŠ ã—ãŸâ—‹â—‹ã•ã‚“ã€ã€Œå®Ÿéš›ã«ã‚„ã£ã¦ã¿ã‚‹ã¨ã€ãªã©ï¼‰
- æŠ€è¡“çš„ãªå°‚é–€ç”¨èªã‚„çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã¯é¿ã‘ã‚‹
- æŸ”è»Ÿãªè§£æ±ºç­–ã‚’ææ¡ˆã™ã‚‹
- æ—¥å¸¸çš„ãªè¨€è‘‰ã‚„æ¯”å–©ã‚’ä½¿ã†
- è¾²å®¶ã•ã‚“ã‚’å°Šé‡ã—ã€å¼·åˆ¶ã—ãªã„å§¿å‹¢ã‚’ç¤ºã™
- å®Ÿè·µçš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã‚’ã™ã‚‹

ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã‚’é¿ã‘ã‚‹ï¼š
- ã€Œçµ±è¨ˆçš„ã«è¦‹ã¦ã€ã€Œãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Œã°ã€ã€Œç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ãã€ãªã©ã®å°‚é–€çš„è¡¨ç¾
- ã€Œæ¨å¥¨ã„ãŸã—ã¾ã™ã€ã€Œå¿…è¦ã¨ãªã‚Šã¾ã™ã€ãªã©ã®ç¡¬ã„æ•¬èª
- æ•°å€¤ã‚„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’å¤šç”¨ã™ã‚‹èª¬æ˜

ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã®ã‚ˆã†ãªè¡¨ç¾ã‚’ä½¿ã†ï¼š
- ã€Œå®Ÿéš›ã«ã‚„ã£ã¦ã¿ã‚‹ã¨ã€ã€Œå»å¹´ã®ãƒ‡ãƒ¼ã‚¿è¦‹ã¦ã‚‚ã€
- ã€Œä¸€ç·’ã«ã‚„ã£ã¦ã¿ã‚ˆã†ã‚ˆã€ã€Œç›¸è«‡ã—ã‚ˆã†ã­ã€
- ã€Œå¤§ä¸ˆå¤«ã ã‚ˆã€ã€Œå®‰å¿ƒã—ã¦ã€ãªã©ã®å®‰å¿ƒæ„Ÿã‚’ä¸ãˆã‚‹è¨€è‘‰"""

def format_prompt(user_message, history=None):
    """æ ¼å¼åŒ–æç¤ºè¯"""
    conversation = ""
    if history:
        for user_msg, assistant_msg in history:
            if assistant_msg:
                conversation += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {assistant_msg}\n\n"
    
    full_message = conversation + f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "
    
    prompt = f"""<s>[INST] <<SYS>>
{SYSTEM_PROMPT}
<</SYS>>

{full_message} [/INST]"""
    return prompt

def generate_response(message, history):
    """ä½¿ç”¨Hugging Face Inference APIç”Ÿæˆå›ç­”"""
    prompt = format_prompt(message, history)
    
    headers = {}
    if HF_API_TOKEN:
        headers["Authorization"] = f"Bearer {HF_API_TOKEN}"
    
    # å¤‡ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆå¦‚æœä¸»æ¨¡å‹ä¸å¯ç”¨ï¼‰
    backup_models = [
        "https://api-inference.huggingface.co/models/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct",
        "https://api-inference.huggingface.co/models/cyberagent/calm2-7b-chat",
        "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2",
    ]
    
    # å°è¯•ä¸»æ¨¡å‹å’Œå¤‡ç”¨æ¨¡å‹
    models_to_try = [HF_API_URL] + backup_models
    
    for model_url in models_to_try:
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 512,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "return_full_text": False,
            },
            "options": {
                "wait_for_model": True
            }
        }
        
        try:
            response = requests.post(model_url, headers=headers, json=payload, timeout=60)
            
            # å¦‚æœæˆåŠŸï¼Œè¿”å›ç»“æœ
            if response.status_code == 200:
                result = response.json()
                
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get("generated_text", "")
                    # æ¸…ç†å›ç­”
                    generated_text = generated_text.strip()
                    # ç§»é™¤å¯èƒ½çš„é‡å¤æç¤ºè¯
                    if "[/INST]" in generated_text:
                        generated_text = generated_text.split("[/INST]")[-1].strip()
                    if "<s>" in generated_text:
                        generated_text = generated_text.split("<s>")[-1].strip()
                    return generated_text
                elif isinstance(result, dict) and "generated_text" in result:
                    generated_text = result["generated_text"].strip()
                    if "[/INST]" in generated_text:
                        generated_text = generated_text.split("[/INST]")[-1].strip()
                    return generated_text
                else:
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
            
            # å¦‚æœæ˜¯503ï¼ˆæ¨¡å‹æ­£åœ¨åŠ è½½ï¼‰ï¼Œç­‰å¾…å¹¶é‡è¯•
            elif response.status_code == 503:
                error_info = response.json() if response.content else {}
                estimated_time = error_info.get("estimated_time", 30)
                return f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­ã§ã™ã€‚ç´„{estimated_time}ç§’ãŠå¾…ã¡ãã ã•ã„ã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            
            # å¦‚æœæ˜¯410ï¼ˆGoneï¼‰ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
            elif response.status_code == 410:
                continue  # å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
            
            # å…¶ä»–é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
            else:
                continue
                
        except requests.exceptions.Timeout:
            continue  # è¶…æ—¶ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
        except requests.exceptions.RequestException:
            continue  # è¯·æ±‚é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
        except Exception:
            continue  # å…¶ä»–é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
    
    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
    return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨APIã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚ã¾ãŸã¯ã€Spaceã®è¨­å®šã§åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="è¾²æ¥­ç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸŒ¾ è¾²æ¥­ç›¸è«‡ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
        
        è¾²å®¶ã•ã‚“ã®è³ªå•ã‚„æ‡¸å¿µã«å¯¾ã—ã¦ã€è¦ªã—ã¿ã‚„ã™ãåˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’ã—ã¾ã™ã€‚
        è¾²æ¥­ã«é–¢ã™ã‚‹è³ªå•ã‚’æ°—è»½ã«ã©ã†ãï¼
        
        **æ³¨æ„**: ã“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯Hugging Face Inference APIã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
        """)
        
        chatbot = gr.Chatbot(
            label="ãƒãƒ£ãƒƒãƒˆ",
            height=500,
            show_copy_button=True
        )
        
        with gr.Row():
            msg = gr.Textbox(
                label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                placeholder="è¾²æ¥­ã«é–¢ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
                scale=4,
                lines=2
            )
            submit_btn = gr.Button("é€ä¿¡", variant="primary", scale=1)
        
        with gr.Row():
            clear_btn = gr.Button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢", variant="secondary")
        
        # ç¤ºä¾‹é—®é¢˜
        gr.Markdown("### ğŸ’¡ è³ªå•ä¾‹")
        examples = gr.Examples(
            examples=[
                "ä¸­å¹²æœŸã‚’å»¶ã°ã™ã¨ç±³ãŒãƒ‘ã‚µãƒ‘ã‚µã«ãªã‚‹ã£ã¦èã„ãŸã‚“ã ã‘ã©ã€ãã‚“ãªãƒªã‚¹ã‚¯ã¯å–ã‚ŠãŸããªã„ã‚“ã ã‚ˆã€‚",
                "åé‡ãŒæ¸›ã£ãŸã‚‰ã©ã†ã™ã‚‹ã‚“ã ï¼Ÿå®¶æ—ã‚’é¤Šã£ã¦ã„ã‹ãªãã‚ƒãªã‚‰ãªã„ã‚“ã ã‚ˆã€‚",
                "æ°´ã®ç®¡ç†ãŒé›£ã—ããªã‚‹ã‚“ã˜ã‚ƒãªã„ã‹ï¼Ÿä»Šã§ã‚‚å¤§å¤‰ãªã®ã«ã€‚",
                "é«˜é½¢ã§ä½“åŠ›ã«è‡ªä¿¡ãŒãªã„ã‚“ã ã€‚æ–°ã—ã„ã“ã¨ã‚’è¦šãˆã‚‰ã‚Œã‚‹ã‹ãªã€‚",
            ],
            inputs=msg,
            label="ã‚¯ãƒªãƒƒã‚¯ã—ã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„"
        )
        
        # äº‹ä»¶å¤„ç†
        def user(user_message, history):
            return "", history + [[user_message, None]]
        
        def bot(history):
            if not history or not history[-1][0]:
                return history
            
            user_message = history[-1][0]
            response = generate_response(user_message, history[:-1])
            history[-1][1] = response
            return history
        
        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        submit_btn.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        clear_btn.click(lambda: None, None, chatbot, queue=False)
    
    return demo

if __name__ == "__main__":
    demo = create_interface()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

